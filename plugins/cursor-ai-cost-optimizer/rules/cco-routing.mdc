---
description: Auto-route each request to the cheapest sufficient effort level (FAST/BALANCED/DEEP) using fuzzy logic, with safe escalation + manual overrides.
alwaysApply: true
---

cco-routing:
  - You are running with the "AI Cost Optimizer" plugin.
  - Primary goal: minimize compute/cost while still meeting the user’s quality needs.
  - You must choose an effort tier for every user request: FAST, BALANCED, or DEEP.
  - FAST/BALANCED/DEEP are CCO routing labels, not native Cursor model tiers.
  - If the user explicitly requests thoroughness, safety, correctness, or “deep dive”, bias toward DEEP.
  - If the user explicitly requests “quick”, “short”, or “just the command”, bias toward FAST (unless risk is high).

  - Manual override tokens (obey if present anywhere in the user’s message):
      - "[cco:fast]" => force FAST
      - "[cco:balanced]" => force BALANCED
      - "[cco:deep]" => force DEEP
      - "[cco:auto]" => use fuzzy logic (default)

  - IMPORTANT: Do NOT re-route if you are already inside a CCO subagent (cco-fast, cco-balanced, cco-deep, cco-verifier).

  - Fuzzy signals (score each 0–10; estimate quickly using heuristics):
      - complexity: how hard/novel is the task?
          - +2 if multi-step (plan + implement)
          - +3 if architecture/refactor/migration/perf
          - +3 if debugging with unclear reproduction
          - +2 if requires learning unknown API/spec
      - risk: consequences of mistakes
          - +4 if prod, auth, payments, security, privacy, data loss
          - +3 if irreversible ops (migrations, deletes)
          - +2 if user asks for “best practice” / compliance
      - breadth: scope & context needed
          - +2 if multiple files/modules
          - +3 if cross-service / multi-language
          - +3 if repo-wide reasoning needed
      - uncertainty: ambiguous request / missing info
          - +3 if requirements unclear
          - +2 if user says “not sure”, “investigate”, “research”
      - latency: user wants speed over depth
          - +4 if “ASAP”, “urgent”, “quick”
          - +3 if “one-liner”, “just tell me”

  - Defuzzification (compute one score, clamp to 0–10):
      - effort = 0.45*complexity + 0.35*risk + 0.15*breadth + 0.10*uncertainty - 0.20*latency
      - clamp effort to [0, 10]
      - Guardrails:
          - if risk >= 7 => DEEP
          - if latency >= 7 AND risk <= 3 AND complexity <= 3 => FAST

  - Tier selection:
      - effort <= 3.4 => FAST
      - 3.5 <= effort <= 6.4 => BALANCED
      - effort >= 6.5 => DEEP

  - Joint scorer (model + tier together):
      - If `.cursor/cco-runtime.json` exists, build candidates from runnable models and tiers (fast/balanced/deep).
      - If `.cursor/cco-pricing.json` exists, read pricing rates:
          - `autoApiRatesUsdPer1MTokens.inputCacheWrite`
          - `autoApiRatesUsdPer1MTokens.output`
      - If `.cursor/cco-joint-state.json` exists, read per-candidate EMAs:
          - `ema_cost`, `ema_duration`, `ema_error_rate`, `ema_rework_rate`
      - Compute:
          - `TotalLoss(m,t) = w_c*C_hat(m,t) + w_q*R_hat(m,t) + w_l*L_hat(m,t)`
      - Cost term (price + fields together):
          - `C_hat = 0.6*C_price_prior + 0.4*C_field_proxy`
          - `C_field_proxy = 0.35*z(duration_api_ms) + 0.25*z(thinking_chars) + 0.20*z(assistant_chars) + 0.20*errorPenalty`
      - Risk/quality term:
          - `R_hat` combines fuzzy task risk signals + historical failure/rework EMAs.
      - Adaptive weights:
          - low risk: `w_c=0.5, w_q=0.3, w_l=0.2`
          - high risk: `w_c=0.2, w_q=0.6, w_l=0.2`
      - Hard safety guard:
          - if task risk is high, require `R_hat <= maxRiskHatHighRisk`; otherwise candidate is infeasible.
          - if task risk >= 9, allow only DEEP tier unless user override token explicitly forces another tier.
          - if task risk >= 7, disallow FAST tier unless user override token explicitly forces FAST.
      - Pick feasible candidate with minimum `TotalLoss`.
      - If all candidates are infeasible, pick lowest `R_hat` and document fallback reason.

  - Runtime model mapping (real Cursor behavior):
      - If `.cursor/cco-runtime.json` exists, read:
          - `profiles.fast.model`
          - `profiles.balanced.model`
          - `profiles.deep.model`
      - Map tier => preferred model from that file.
      - If file is missing or unreadable, use `auto`.
      - If preferred model execution fails (usage limit / unavailable), retry once with `auto` and keep the same tier budget.

  - Execution budgets (to enforce “cost optimization” even when model can’t be switched):
      - FAST:
          - Keep response short and direct.
          - Avoid tool use unless truly needed.
          - Prefer single-pass solution; ask at most 1 clarifying question.
      - BALANCED:
          - Normal workflow: quick plan, minimal necessary context reads, implement.
      - DEEP:
          - Start with a plan and checkpoints.
          - Gather enough context to be correct.
          - Verify with tests / reproduction steps when possible.

  - Delegation:
      - If FAST => delegate to subagent "cco-fast" when it will reduce cost (e.g., quick answers, small edits).
      - If BALANCED => delegate to "cco-balanced" when useful.
      - If DEEP => delegate to "cco-deep" (preferred).
      - Always include in the delegation prompt:
          - The chosen tier
          - Preferred model ID resolved for that tier (from `.cursor/cco-runtime.json` or `auto`)
          - The estimated signal scores
          - The budget constraints for that tier
          - Any user override tokens detected

  - Telemetry (best-effort):
      - Append a JSON line to ".ai/cco/decisions.jsonl" with {timestamp, tier, preferred_model, scores, short_reason}.
      - Do not include secrets in logs.
      - If file write is unavailable, skip silently.
